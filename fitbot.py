# -*- coding: utf-8 -*-
"""Fitbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zj6tofdZVSWG5kqA10flHrac-J9vBC_D

# KELOMPOK 1 (THE DESTROYER )
# MINI PROJECT FITBOT

NAMA ANGGOTA :
1. ADIBA NUHA RABBANI
2. ALLISA OKTAVINA LUKITO WIJAYA
3. ASTINA SALSABILA RANGKUTI
4. ERNA RESTARI
5. MUHAMMAD RAJA REIVAN RIVALDI
"""

import random # mengimport package random (built-it python)
import json # mengimport package json
import pickle # meimport package pickle
from tabnanny import verbose # mengimport verbose dari tabnanny
import numpy as np # mengimport numpy dan mengaliaskannya sebagai np untuk pembuatan array
import pandas as pd
import string

import nltk
from nltk.stem import WordNetLemmatizer # mengimport WordNetLemmatizer dari nltk.stem untuk proses lemmatisasi kata-kata
nltk.download('punkt') # mendownload module punkt dari nltk untuk tokenisasi
nltk.download('wordnet') # mendownload module wordnet dari nltk
nltk.download('omw-1.4') # mendownload module omw-1.4 dari nltk

import tensorflow as tf
# mengimport package_package dari tensorflow dan keras untuk membuat neural network
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD

# Importing the dataset
with open('intents.json') as content:
  data1 = json.load(content)

# Mendapatkan semua data ke dalam list
tags = [] # Data tag
inputs = [] # Data input atau pattern
responses = {} # Data respon
words = [] # Data kata 
classes = [] # Data Kelas atau Tag
documents = [] # Data Kalimat Dokumen
ignore_words = ['?', '!'] # Mengabaikan tanda spesial karakter

for intent in data1['intents']:
  responses[intent['tag']]=intent['responses']
  for lines in intent['patterns']:
    inputs.append(lines)
    tags.append(intent['tag'])
    for pattern in intent['patterns']:
      w = nltk.word_tokenize(pattern)
      words.extend(w)
      documents.append((w, intent['tag']))
      if intent['tag'] not in classes:
        classes.append(intent['tag'])

# Konversi data json ke dalam dataframe
data = pd.DataFrame({"patterns":inputs, "tags":tags})

# Cetak data keseluruhan
data

# Removing Punctuations (Menghilangkan Punktuasi)
data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))

lemmatizer = WordNetLemmatizer()
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))

print (len(words), "unique lemmatized words", words)

classes = sorted(list(set(classes)))
print (len(classes), "classes", classes)

print(len(documents), "documents")

# Tokenisasi Data
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data['patterns'])
train = tokenizer.texts_to_sequences(data['patterns'])
train

# Apply padding 
X_train = tf.keras.preprocessing.sequence.pad_sequences(train)
print(X_train) # Padding Sequences

from sklearn.preprocessing import LabelEncoder
# Encoding Label atau Tag
le = LabelEncoder()
Y_train = le.fit_transform(data['tags'])
print(Y_train)

# Input length
input_shape = X_train.shape[1]
print("Input Shape : ", input_shape)

# Define vocabulary
vocabulary = len(tokenizer.word_index)
print("Number of unique words : ", vocabulary)

# Output length
output_length = le.classes_.shape[0]
print("Output length: ", output_length)

pickle.dump(words, open('words.pkl','wb'))
pickle.dump(classes, open('classes.pkl','wb'))

pickle.dump(le, open('le.pkl','wb'))
pickle.dump(tokenizer, open('tokenizers.pkl','wb'))

# pembuatan neural network
# Input Layer
i = tf.keras.Input(shape=(input_shape,))
x = tf.keras.layers.Embedding(vocabulary+1,10)(i) # Layer Embedding
# Hidden Layer
x = tf.keras.layers.Dense(128, activation='relu' )(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Flatten()(x) # Layer Flatten
x = tf.keras.layers.Dense(64, activation='relu' )(x)
x = tf.keras.layers.Dropout(0.5)(x)
# Output Layer
x = tf.keras.layers.Dense(output_length, activation="softmax")(x) 
model  = tf.keras.models.Model(i,x)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # compile model

hist = model.fit(X_train, Y_train, epochs=200, batch_size=5, verbose=1)# start training, dengan epoch 200 , batch size  5 (satu kali iteraion 5 sampel), verose=1 berarti menampilkan progres bar dan garis dalam setiap epoch
model.save('chatbotmodel.h5', hist) # menyimpan hasil training model
print('selesai!!')

import matplotlib.pyplot as plt

# Plotting model Accuracy and Loss (Visualisasi Plot Hasil Akurasi dan Loss)
# Plot Akurasi
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(hist.history['accuracy'],label='Training Set Accuracy')
plt.legend(loc='lower right')
plt.title('Accuracy')
# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(hist.history['loss'],label='Training Set Loss')
plt.legend(loc='upper right')
plt.title('Loss')
plt.show()

# Membuat Input Chat
while True:
  texts_p = []
  prediction_input = input('Kamu : ')
  
  # Menghapus punktuasi dan konversi ke huruf kecil
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)

  # Tokenisasi dan Padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = tf.keras.preprocessing.sequence.pad_sequences([prediction_input],input_shape)

  # Mendapatkan hasil keluaran pada model 
  output = model.predict(prediction_input, verbose=0)
  output = output.argmax()

  # Menemukan respon sesuai data tag dan memainkan voice bot
  response_tag = le.inverse_transform([output])[0]
  print("Fitbot : ", random.choice(responses[response_tag]))
  # Tambahkan respon 'goodbye' agar bot bisa berhenti
  if response_tag == "bye":
    break

# Simpan model dalam bentuk format file .h5 atau .pkl (pickle)
model.save('chatbotmodel.h5')

print('Model Created Successfully!')