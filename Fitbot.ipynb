{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95bjPcuvMy9o"
      },
      "source": [
        "# KELOMPOK 1 (THE DESTROYER )\n",
        "# MINI PROJECT FITBOT\n",
        "\n",
        "NAMA ANGGOTA :\n",
        "1. ADIBA NUHA RABBANI\n",
        "2. ALLISA OKTAVINA LUKITO WIJAYA\n",
        "3. ASTINA SALSABILA RANGKUTI\n",
        "4. ERNA RESTARI\n",
        "5. MUHAMMAD RAJA REIVAN RIVALDI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLIf9hoHNhs8",
        "outputId": "a14abc21-cdc9-4850-ecf8-e39ef4af3f0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "24/24 [==============================] - 1s 2ms/step - loss: 3.4758 - accuracy: 0.0924 \n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 3.3949 - accuracy: 0.0924\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 3.3362 - accuracy: 0.1513\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 3.2295 - accuracy: 0.1597\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 3.0633 - accuracy: 0.2185\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 3.1021 - accuracy: 0.1849\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.8600 - accuracy: 0.2605\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.7424 - accuracy: 0.2521\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.5300 - accuracy: 0.3277\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.4236 - accuracy: 0.2941\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.1917 - accuracy: 0.4202\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 2.2097 - accuracy: 0.3277\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.7284 - accuracy: 0.5546\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.7830 - accuracy: 0.5042\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.7167 - accuracy: 0.5714\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.5000 - accuracy: 0.5882\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.4984 - accuracy: 0.5882\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.3752 - accuracy: 0.5798\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.3197 - accuracy: 0.6303\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.2478 - accuracy: 0.6723\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.1443 - accuracy: 0.6723\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.1358 - accuracy: 0.6218\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.9648 - accuracy: 0.7143\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.0186 - accuracy: 0.6891\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.8390 - accuracy: 0.7815\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.8978 - accuracy: 0.7059\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.7653 - accuracy: 0.7731\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.7647\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.8702 - accuracy: 0.7395\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.7899\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.8571\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.7983\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.7563\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.7899\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.7983\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.8571\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5880 - accuracy: 0.8235\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8655\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4145 - accuracy: 0.8655\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.8739\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.8739\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.8487\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8571\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8824\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8739\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8739\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8487\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8824\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.9076\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8908\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8487\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8739\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8824\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8824\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5230 - accuracy: 0.8403\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8908\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.9076\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8571\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.9076\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.8235\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.9076\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.9076\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3621 - accuracy: 0.8571\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.8235\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.9076\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8908\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8908\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8571\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8824\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8992\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.9076\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8992\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.9076\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8992\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.8824\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9328\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8655\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.8992\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.9244\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.8319\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8992\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.9076\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8739\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8739\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8739\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8655\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.9076\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2181 - accuracy: 0.9076\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.9076\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8739\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2811 - accuracy: 0.9160\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.9160\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9412\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9076\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.9328\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2702 - accuracy: 0.9160\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2446 - accuracy: 0.8992\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.9412\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.9244\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9160\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9328\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9328\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 0.9244\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9412\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8992\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.9076\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1956 - accuracy: 0.9412\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.9412\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8992\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9328\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9160\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9160\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.9244\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.8908\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9412\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9244\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9160\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9244\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8992\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9076\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.9160\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.9244\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8908\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1850 - accuracy: 0.9496\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9244\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9412\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9160\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9076\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9328\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9412\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1634 - accuracy: 0.9244\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9244\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9412\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1884 - accuracy: 0.9412\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9664\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9496\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1559 - accuracy: 0.9412\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9076\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9580\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9328\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9244\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9580\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2111 - accuracy: 0.9496\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.9160\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.9076\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9244\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9412\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9328\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.9496\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8992\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9244\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9244\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1862 - accuracy: 0.9244\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.9244\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.9412\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1466 - accuracy: 0.9580\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9244\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9496\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9412\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.8992\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9244\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9160\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1696 - accuracy: 0.9412\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9244\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9076\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9496\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9244\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1075 - accuracy: 0.9748\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9496\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9328\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9412\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9580\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9580\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.9412\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.9580\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.8992\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9664\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9412\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9664\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9244\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9328\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9496\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9160\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9496\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9328\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9496\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9244\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9412\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9160\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9412\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9580\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9160\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9160\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9244\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9580\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9412\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9244\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9496\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9412\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9580\n",
            "selesai!!\n"
          ]
        }
      ],
      "source": [
        "import random # mengimport package random (built-it python)\n",
        "import json # mengimport package json\n",
        "import pickle # meimport package pickle\n",
        "from tabnanny import verbose # mengimport verbose dari tabnanny\n",
        "import numpy as np # mengimport numpy dan mengaliaskannya sebagai np untuk pembuatan array\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer # mengimport WordNetLemmatizer dari nltk.stem untuk proses lemmatisasi kata-kata\n",
        "nltk.download('punkt') # mendownload module punkt dari nltk untuk tokenisasi\n",
        "nltk.download('wordnet') # mendownload module wordnet dari nltk\n",
        "nltk.download('omw-1.4') # mendownload module omw-1.4 dari nltk\n",
        "\n",
        "# mengimport package_package dari tensorflow dan keras untuk membuat neural network\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# codingan ini menampilkan hasil tensorflow model, layer, dan optimizer dengan mengimport sequential, dense, activation, dropout, &SGD.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() # menampung function WordNetLemmatizer() ke dalam variabel lemmatizer\n",
        "\n",
        "intents = json.loads(open('intents.json').read()) # load dan read isi file intens.json\n",
        "\n",
        "words = [] # variabel berisi list kosong untuk kemudian menampung hasil dari for loop di bawah\n",
        "classes = [] # variabel berisi list kosong untuk kemudian menampung hasil dari for loop di bawah\n",
        "documents = [] # variabel berisi list kosong untuk kemudian menampung hasil dari for loop di bawah\n",
        "ignore_letters = ['?','!',',','.'] # list untuk tanda baca yang diabaikan\n",
        "\n",
        "for intent in intents['intents']: # for loop untuk setiap item yang ada pada file intent.json\n",
        "    for pattern in intent['patterns']: # nested for loop untuk setiap pattern yang ada pada intents.json\n",
        "        word_list = nltk.word_tokenize(pattern) # menampung function untuk tokenisasi pattern ke dalam variabel word_list\n",
        "        words.extend(word_list) # menambahkan/memperpanjang isi list dalam variabel words dengan item di variabel word_list\n",
        "        documents.append((word_list, intent['tag'])) # menambahkan variabel documents yang berupa list kosong dengan word_lsi dan tag dalam file intents.json\n",
        "        if intent['tag'] not in classes: # membuat kondisi jika tag dalam intents.json tidak berada dalam classes\n",
        "            classes.append(intent['tag']) # menambahkan variabel classes yang berupa list kosong dengan tag yang ada dalam intents.json\n",
        "\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters ] # menampung function untuk melematisasi tiap-tiap kata yang ada pada variabel words syaratnya jika kata-kata tersebut tidak ada pada variabel ignore_letters\n",
        "words = sorted(set(words)) # merubah kembali menjadi list dan menghilangkan duplikasi\n",
        "\n",
        "#melakukan short classes \n",
        "classes = sorted(set(classes)) # merubah kembali menjadi list dan menghilangkan duplikasi\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb')) # serialisasi, mengubah isi dari words menjadi words.pkl dalam bentuk byte stream (mode binary)\n",
        "pickle.dump(classes, open('classes.pkl', 'wb')) # serialisasi,  mengubah isi dari classes menjadi classes.pkl dalam bentuk byte stream (mode binary)\n",
        "\n",
        "#Membuat Training dan Testing Data Untuk model Train, setiap pola akan diubah dan diinput menjadi angka.\n",
        "#membuat training data \n",
        "training = [] # variabel kosong untuk menampung hasil dari codingan untuk training di bawah\n",
        "output_empty = [0] * len(classes) # menampung list angka 0 sebanyak panjang item yang ada dalam classes ke dalam variabel output_empty\n",
        "\n",
        "# pembuatan bag of words untuk persiapan training\n",
        "for document in documents: # for loop untuk tiap-tiap item pada variabel documets\n",
        "    bag = [] # membuat variabel bag yang berisikan list kosong\n",
        "    word_patterns = document[0] # pembuatan variabel word_patterns\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words: #membuat bag of word array dengan 1 \n",
        "        bag.append(1) if word in word_patterns else bag.append(0) # menambahkan variabel bag yang berisi list dengan angka 1 jika sebuah berada dalam variabel word_patterns jika tidak tambahkan 0 \n",
        "\n",
        "    output_row = list(output_empty) # copylist dari variabel output_empty untuk variabel output_row\n",
        "    output_row[classes.index(document[1])] = 1 # menambahkan angka 1 untuk setiap kata yang benar berada dalam suatu dokumen (0 berarti tidak ada kata tertentu dalam dokumen tertentu)\n",
        "    training.append([bag, output_row])\n",
        "#shuffle features dan membuat numpy array \n",
        "random.shuffle(training)\n",
        "training = np.array(training) # mengubah variabel training menjadi numpy array\n",
        "#memisahkan feature dan label untuk training\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "\n",
        "# pembuatan neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu' ))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) # compile model\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)# start training, dengan epoch 200 , batch size  5 (satu kali iteraion 5 sampel), verose=1 berarti menampilkan progres bar dan garis dalam setiap epoch\n",
        "model.save('chatbotmodel.h5', hist) # menyimpan hasil training model\n",
        "print('selesai!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBoj3hk2OeMp",
        "outputId": "112b2725-6f64-4e34-b2f5-b9e5dee43c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is up!\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Halo! Saya Fitbot, salam kenal ya! Mau tau tentang diet, kan?\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Halo! Saya Fitbot, salam kenal ya! Mau tau tentang diet, kan?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "non-exercise activity thermogenesis (NEAT) adalah semua aktifitas fisik yang kamu lakukan setiap hari yang tidak dimaksudkan sebagai bentuk olahraga, misalnya berjalan kaki ke tempat tujuan, berdiri, naik tangga, mengerjakan pekerjaan rumah, berkebun, bermain bersama anak, dan lain-lain.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "diet tidak perlu bermodalkan uang yang banyak, asal kamu tahu caranya untuk memilih menu dan bahan makanan yang murah. seperti contohnya dada ayam, buah, dan sayuran masih terbilang terjangkau harganya setelah itu barulah siasati bagaimana cara mengolahnya agar sehat, enak, dan rendah kalori\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "puasa sendiri memiliki banyak manfaat kesehatan menurut science, tetapi yang jadi penentu apakah kamu dapat menurunkan berat badan atau tidak adalah kalori defisit, jika saat puasa kamu malah kelebihan kalori (surplus) maka alih-alih mengurangi kadar lemak malah membuat kamu semakin berlemak.\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "diet tidak perlu bermodalkan uang yang banyak, asal kamu tahu caranya untuk memilih menu dan bahan makanan yang murah. seperti contohnya dada ayam, buah, dan sayuran masih terbilang terjangkau harganya setelah itu barulah siasati bagaimana cara mengolahnya agar sehat, enak, dan rendah kalori\n"
          ]
        }
      ],
      "source": [
        "import random #perintah untuk mengimport library random\n",
        "import json #perintah untuk mengimport library json\n",
        "import pickle #perintah untuk mengimport library pickle\n",
        "import numpy as np #perintah untuk mengimport library numpy as np\n",
        "import nltk #perintah untuk mengimport library nltk\n",
        "from keras.models import load_model # mengimport load_model dari keras.models \n",
        "from nltk.stem import WordNetLemmatizer # mengimport package WordNetLemmatizer untuk lemmatisasi kata\n",
        "#codingan ini menggunakan lemmatizer yang bertujuan untuk menormalisasi pada teks \n",
        "lemmatizer = WordNetLemmatizer() # memasukkan function WordNetLemmatizer() ke dalam variabel lemmatizer\n",
        "intents = json.loads(open(\"/content/intents.json\").read()) # membuka file intents.json (dataset)\n",
        "words = pickle.load(open('words.pkl', 'rb')) # membuka file words.pkl dalam format binary\n",
        "classes = pickle.load(open('classes.pkl', 'rb')) # membuka file classes.pkl dalam format binary\n",
        "model = load_model('chatbotmodel.h5') # load model yang sudah ditraining\n",
        "\n",
        "\n",
        "# merupakan fungsi yang mengambil input pengguna untuk proses tokenisasi dan lemmatisasi\n",
        "def clean_up_sentences(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word)\n",
        "                      for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# function yang mengolah input user menjadi bag of words\n",
        "def bagw(sentence):\n",
        "    sentence_words = clean_up_sentences(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag) #karena membutuhkan fitur nomerik untuk memprediksi kelas\n",
        "    #maka perintah diatas berfungsi untuk membuat model bag of words untuk teks yang telah di proses sebelumnya\n",
        "  \n",
        "def predict_class(sentence): # function untuk memprediksi kelas atau tag dari pertanyaan yang diajukan oleh pengguna\n",
        "    bow = bagw(sentence)\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res)\n",
        "               if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({'intent': classes[r[0]],\n",
        "                            'probability': str(r[1])})\n",
        "        return return_list\n",
        "  \n",
        "def get_response(intents_list, intents_json): # function untuk memilih respon acak dari tag yang di prediksi dan mengirimkannya sebagai respons bot\n",
        "    tag = intents_list[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    result = \"\"\n",
        "    for i in list_of_intents:\n",
        "        if i['tag'] == tag:\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "  \n",
        "print(\"Fitbot siap melayani!\") # menandakan chatbot siap digunakan\n",
        "\n",
        "# while loop yang berarti program akan terus berjalan selama user berinteraksi dengan chatbot\n",
        "while True:\n",
        "    message = input(\"\") # input untuk user\n",
        "    ints = predict_class(message) # menerapkan function predict_class dengan parameter message, yang mana message adalah input user (mengolah input user dengan function predict_class)\n",
        "    res = get_response(ints, intents) # menampilkan respon yang relevan dengan input dari user menggunakan function get_response\n",
        "    print(res) # menampilkan respon dari chatbot"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
